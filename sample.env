# Copy to .env on the GPU VM and edit values as needed

# Optional: Hugging Face token for gated/private models
HF_TOKEN=

# Optional: Shared model cache directory on the VM
MODEL_CACHE=/data

# -------- Server 1 (defaults to port 8000) --------
NAME1=llm1
MODEL1=aisingapore/Gemma-SEA-LION-v3-9B-IT
PORT1=8000
# Optional tuning
TP1=1
DTYPE1=bfloat16
# Max sequence length (reduced from 8192 for memory efficiency)
# Max parallel sequences (reduced from 256 for memory efficiency)  
# GPU memory utilization (95% for large models)
MAX_MODEL_LEN1=2048
MAX_NUM_SEQS1=64
GPU_MEMORY_UTILIZATION1=0.95
# Optional: pin to a specific GPU (e.g., 0)
CUDA_VISIBLE_DEVICES1=0

# -------- Server 2 (defaults to port 8001) --------
NAME2=llm2
MODEL2=NanEi/fr_sealion_merge_bot_v1-5
PORT2=8001
# Optional tuning
TP2=1
DTYPE2=bfloat16
# Max sequence length (reduced from 8192 for memory efficiency)
# Max parallel sequences (reduced from 256 for memory efficiency)
# GPU memory utilization (95% for large models)
MAX_MODEL_LEN2=2048
MAX_NUM_SEQS2=64
GPU_MEMORY_UTILIZATION2=0.95
# Optional: pin to a specific GPU (e.g., 1)
CUDA_VISIBLE_DEVICES2=1

# -------- Server 3 (defaults to port 8002) --------
NAME3=llm3
MODEL3=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
PORT3=8002
# Optional tuning
TP3=1
DTYPE3=float16
# Max sequence length (small for sentence-transformers)
# Max parallel sequences (small for sentence-transformers)
# GPU memory utilization (2% for sentence-transformers)
MAX_MODEL_LEN3=128
MAX_NUM_SEQS3=32
GPU_MEMORY_UTILIZATION3=0.02
# No GPU pinning needed for this smaller model


