# Copy to .env on the GPU VM and edit values as needed

# Optional: Hugging Face token for gated/private models
HF_TOKEN=

# Optional: Shared model cache directory on the VM
LLM_MODEL_CACHE=/home/ubuntu/.cache/huggingface

# -------- Server 1 (defaults to port 8000) --------
LLM_NAME1=llm1
LLM_MODEL1=aisingapore/Gemma-SEA-LION-v3-9B-IT
LLM_PORT1=8000
# Optional tuning
LLM_TP1=1
LLM_DTYPE1=float16
LLM_MAX_MODEL_LEN1=8192
# Optional: pin to a specific GPU (e.g., 0)
LLM_CUDA_VISIBLE_DEVICES1=0

# -------- Server 2 (defaults to port 8001) --------
LLM_NAME2=llm2
LLM_MODEL2=NanEi/fr_sealion_merge_bot_v1-5
LLM_PORT2=8001
# Optional tuning
LLM_TP2=1
LLM_DTYPE2=float16
LLM_MAX_MODEL_LEN2=8192
# Optional: pin to a specific GPU (e.g., 1)
LLM_CUDA_VISIBLE_DEVICES2=1

# -------- Server 3 (defaults to port 8002) --------
LLM_NAME3=llm3
LLM_MODEL3=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
LLM_PORT3=8002
# Optional tuning
LLM_TP3=1
LLM_DTYPE3=float16
LLM_MAX_MODEL_LEN3=8192
# No GPU pinning needed for this smaller model


