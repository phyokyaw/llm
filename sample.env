# Copy to .env on the GPU VM and edit values as needed

# Optional: Hugging Face token for gated/private models
HF_TOKEN=

# Optional: Shared model cache directory on the VM
VM_MODEL_CACHE=/home/ubuntu/.cache/huggingface

# -------- VM1 server (defaults to port 8000) --------
VM1_NAME=vm1
VM1_MODEL=meta-llama/Llama-3.1-8B-Instruct
VM1_PORT=8000
# Optional tuning
VM1_TP=1
VM1_DTYPE=float16
VM1_MAX_MODEL_LEN=8192

# -------- VM2 server (defaults to port 8001) --------
VM2_NAME=vm2
VM2_MODEL=mistralai/Mistral-7B-Instruct-v0.3
VM2_PORT=8001
# Optional tuning
VM2_TP=1
VM2_DTYPE=float16
VM2_MAX_MODEL_LEN=8192


